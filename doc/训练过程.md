#　特征预处理
第一步需要将棋盘的信息抽象称为特征，输入到神经网络中。
* 所有的特征都采用“one-hot”编码的方式输入。
* 围棋中的liberty指的是气，采用ont-hot编码气，如果一个子有1个气，那么就是“10000000”；如果有4个子，那么就是“00010000”。
* 通过AlphaGo.preprocessing.game_converter来进行转换，主要生成三个类型的数据
  * states数据集：n_data x feature_planes x board_width x board_height
  * actions数据集：n_data x 2，存储对应的落子
  * file_offsets组：

#　第１阶段：采用监督学习训练策略网络
采用卷积神经网络来预测可能的落子位置，采用SGD和反向传播算法来最大化指定落子位置的概率。
* 第一步，生成模型配置
```python
from AlphaGo.models.policy import CNNPolicy
arch = {'filters_per_layer': 128, 'layers': 12} # args to CNNPolicy.create_network()
features = ['board', 'ones', 'turns_since'] # Important! This must match args to game_converter
policy = CNNPolicy(features, **arch)
policy.save_model('my_model.json')
```
* 第二步，运行训练程序
```sh
$ python -m AlphaGo.training.supervised_policy_trainer my_model.json debug_feature_planes.hdf5 training_results/ --epochs 5 --minibatch 32 --learning-rate 0.01
```
# 第2阶段: 采用强化学习，来增加训练样本
监督学习一般为强化学习提供初始的参数。
* 更新权重的标准是自己和自己下棋后是否赢了。如果赢了就更新，否则就不更新。

# 第3阶段：采用强化学习训练价值网络
价值网络用来判断游戏的胜负
* 这个网络的最终目标是让预测结果和真实结果的误差最小，预测胜利的为（+1），预测失败为（-1）

# 第4阶段：重新进行第2阶段
